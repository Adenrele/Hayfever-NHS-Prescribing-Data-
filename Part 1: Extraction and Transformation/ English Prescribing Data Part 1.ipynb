{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHSBSA English Prescribing Data (EPD) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                            Part 1: Data Extraction and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API documentation https://docs.ckan.org/en/2.8/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the requests module. This module is used to send HTTP requests in python.\n",
    "import requests  \n",
    "\n",
    "#The url below is described within the API documentation and is used to view the datasets available.\n",
    "url = \"http://opendata.nhsbsa.net/api/3/action/package_list\"\n",
    "\n",
    "#Response.get sends a get request to obtain data from a url.\n",
    "response = requests.get(url)\n",
    "\n",
    "#The status code below is used to ensure the request has/can be made successfully.\n",
    "#Status code 200 means the request was successful.\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the response in dictionary format.\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the values in the 'result' key.\n",
    "#The 'result' key contains the names of datasets available.\n",
    "\n",
    "#Only three datasets are available.\n",
    "print(response.json()['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the API documentation, the url containing the medicines data was used and tested using response.status_code\n",
    "url2 = \"http://opendata.nhsbsa.net/api/3/action/package_show?id=english-prescribing-data-epd\"\n",
    "response2 = requests.get(url2)\n",
    "response2_json = response2.json() \n",
    "response2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The keys within the output of the results were investigated.\n",
    "response2_json['result'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each dataset for medicines in secondary care is stored within csv files downloadable through a url.\n",
    "response2_json['result']['resources'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets =[]\n",
    "for num in range(len(response2_json['result']['resources'])):\n",
    "    datasets.append(response2_json['result']['resources'][num]['url'])\n",
    "#datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019's prescribing data can now be found in datasets[60:72], however, an attempt to read all of the CSVs into memory using Pandas will result in wastage of hours followed by a crash.\n",
    "\n",
    "An option is to read the CSVs in chunks, however, I soon learnt that there just is not enough memory to complete the task in a manner which is efficient and allows for reproduction. My Macbook possesses a meagre 8GB of memory. \"Restart and Run All Cells\" became synonymous with \"waste the whole weekend\".\n",
    "\n",
    "The solution? Read only the features and records which are required. There is also a need to investigate the data types which take up less memory and apply necessary changes to features whilst reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, an entire CSV (one month) is read to memory. The size can be seen below.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(datasets[60], sep =\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info(memory_usage='deep', null_counts=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's free up that memory!\n",
    "\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, a full reading of only one CSV assumes  MB of memory. This is not efficient. Below a function which reads a list of urls in Pandas whilst preparing the data to use memory effificiently is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def LargeCSVsChop(listofCSV_URLs):\n",
    "    \n",
    "    \n",
    "    dfs = []\n",
    "    for num in range(len(listofCSV_URLs)):\n",
    "\n",
    "        feats = ['YEAR_MONTH', 'PCO_NAME', 'PRACTICE_NAME', 'CHEMICAL_SUBSTANCE_BNF_DESCR', \n",
    "                'BNF_DESCRIPTION', 'TOTAL_QUANTITY', 'ACTUAL_COST']\n",
    "        \n",
    "        custom_date_parser = lambda x: datetime.strptime(x, \"%Y%m\")\n",
    "        \n",
    "        df = pd.read_csv(listofCSV_URLs[num], sep =\",\", usecols = feats, date_parser=custom_date_parser,\n",
    "                                                                         parse_dates=['YEAR_MONTH'])\n",
    "        \n",
    "        \"\"\"\"\n",
    "        'feats' is a list of the columns/features from the CSV that I want Pandas to read to memory.\n",
    "        \n",
    "        types = {'TOTAL_QUANTITY': int, 'ACTUAL_COST':float}\n",
    "        \n",
    "        Types is a dictionary of columns and the datatypes they should be read as using dtypes = types in the read_csv function. \n",
    "        \n",
    "        The types are optimal so no changes will be made but the dictionary can be used in future.    \n",
    "        \n",
    "        \n",
    "        'custom_data_parser' will be used to convert the dtype of the YEAR_MONTH feature to date format.\n",
    "\n",
    "        \n",
    "        \n",
    "        The dataframe will read only feats and convert the dtype of the YEAR_MONTH feature.\n",
    "        \n",
    "        'df' is used as a variable again for the filtered dataframe to prompt the deallocation of the unfiltered\n",
    "        #dataframe object from memory by dropping the reference count to zero and prompting garbage collection algorithms.\n",
    "        \n",
    "        \n",
    "        Oral antihistamines will be the focus of this analysis. 'CHEMICAL_SUBSTANCE_BNF_DESCR' is used to ensure all\n",
    "        forms and brads of the drugs are captured. Further work is done using 'BNF_DESCRIPTION' to remove unwanted products.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df = df[ \n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Cetirizine hydrochloride')                              | \n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Loratadine')                                            |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Desloratadine')                                         |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Fexofenadine hydrochloride')                            |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Acrivastine')                                           |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Bilastine')                                             |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Levocetirizine')                                        |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Mizolastine')                                           |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Chlorphenamine maleate')                                & \n",
    "               (df['BNF_DESCRIPTION']!='Chlorphenamine 10mg/1ml solution for injection ampoules')            |\n",
    "               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Promethazine hydrochloride')                            &\n",
    "               (df['BNF_DESCRIPTION']!='Promethazine 25mg/1ml solution for injection ampoules')              &\n",
    "               (df['BNF_DESCRIPTION']!='Phenergan 25mg/1ml solution for injection ampoules')                   \n",
    "             ]\n",
    "      \n",
    "        dfs.append(df)\n",
    "        \n",
    "    df_large = pd.concat(dfs)\n",
    "        \n",
    "    return df_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dataframe read this way uses a lot less memory (13MB) but another huge problem now exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan_2019_df = LargeCSVsChop(datasets[60:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan_2019_df.info(memory_usage='deep', null_counts=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue of time is partially solved by reading only the features required for the dataframe as demonstrated below. Hoever, bandwidth is another limitation, this work requires pandas to parce a CSV though a URL; low speeds will ensure the process is slow regardless of how (in)efficient this process is for Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def timefunc(function, arg, repeats = 20):\n",
    "    \n",
    "    alltime = []\n",
    "    \n",
    "    while  repeats > 0:\n",
    "        \n",
    "        \"\"\"\n",
    "        Unless specified the number of repitions will be 20.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        starttime= time.time() # record the start time\n",
    "\n",
    "        result = function(arg) # run the function and store in the variable 'result' in case result is needed.\n",
    "\n",
    "        endtime = time.time() # Record end time.\n",
    "\n",
    "        timetaken = endtime - starttime \n",
    "        \n",
    "        alltime.append(timetaken) \n",
    "        \n",
    "        repeats -=1  \n",
    "        \n",
    "    mean = np.mean(alltime) #Find the mean.\n",
    "    std = np.std(alltime) #Find the standard deviation.\n",
    "    error=std/(len(alltime)**0.5)   #Find the standard error.\n",
    "    \n",
    "    return (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FuncReadCSV(listofdataframes):\n",
    "    \n",
    "    frames = []\n",
    "    for csv in listofdataframes:\n",
    "        df = pd.read_csv(csv)\n",
    "        frames.append(df)\n",
    "        dfs= pd.concat(frames)\n",
    "    return df\n",
    "\n",
    "Without_ncols = timefunc(FuncReadCSV, datasets[60:62], repeats = 1)\n",
    "\n",
    "#Return the mean timetaken to run read a list of CSVs without removing columns or filtering records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LargeCSVsChop_MINI(listofCSV_URLs):\n",
    "    \n",
    "    \"\"\"\n",
    "    The same as LargeCSVsChop without the filtering element. This is to enable like for like\n",
    "    comparisons as much as possible (with the exception of date parsing) as it pertains to time\n",
    "\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for num in range(len(listofCSV_URLs)):\n",
    "\n",
    "        feats = ['YEAR_MONTH', 'PCO_NAME', 'PRACTICE_NAME', 'CHEMICAL_SUBSTANCE_BNF_DESCR', \n",
    "                'BNF_DESCRIPTION', 'TOTAL_QUANTITY', 'ACTUAL_COST']\n",
    "        \n",
    "        custom_date_parser = lambda x: datetime.strptime(x, \"%Y%m\")\n",
    "        \n",
    "        df = pd.read_csv(listofCSV_URLs[num], sep =\",\", usecols = feats, date_parser=custom_date_parser,\n",
    "                                                                         parse_dates=['YEAR_MONTH'])\n",
    "      \n",
    "        dfs.append(df)\n",
    "        \n",
    "    df_large = pd.concat(dfs)\n",
    "        \n",
    "    return df_large\n",
    "\n",
    "\n",
    "\n",
    "With_ncols = timefunc(LargeCSVsChop_MINI, datasets[60:62], repeats = 1)\n",
    "\n",
    "#Return the mean time taken to read one CSV using the LargeCSVsChop_MINI function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax1=plt.subplots(figsize=(10,6))\n",
    "\n",
    "plt.xlabel('CSV Reading Methods')\n",
    "plt.ylabel('Mean Time Taken to Read (minutes)')\n",
    "plt.title('Comparison of Time Taken to Read Two CSVs ')\n",
    "\n",
    "plt.yticks(range(100))\n",
    "            \n",
    "plt.bar(height=Without_ncols/60,x = 'Without Function', color = 'red')\n",
    "plt.bar(height=With_ncols/60,x='LargeCSVsChop_MINI', color = 'blue')\n",
    "plt.savefig('Time taken')\n",
    "\n",
    "#Bearing in mind that the LargeCSVsChop function doesn't just read to dataframe. It also parses the YEAR_MONTH column to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole year of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each csv is read in individual lines as opposed to one to allow for fleixibility when the kernel needs to be interrupted due to the time taken or any other issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feb_2019_df = LargeCSVsChop(datasets[61:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mar_2019_df = LargeCSVsChop(datasets[62:63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apr_2019_df = LargeCSVsChop(datasets[63:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "May_2019_df = LargeCSVsChop(datasets[64:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jun_2019_df = LargeCSVsChop(datasets[65:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jul_2019_df = LargeCSVsChop(datasets[66:67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_2019_df = LargeCSVsChop(datasets[67:68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sep_2019_df = LargeCSVsChop(datasets[68:69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Oct_2019_df = LargeCSVsChop(datasets[69:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nov_2019_df = LargeCSVsChop(datasets[70:71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dec_2019_df = LargeCSVsChop(datasets[71:72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HayFev_df_2019_list = [Jan_2019_df,Feb_2019_df ,Mar_2019_df,Apr_2019_df,May_2019_df,Jun_2019_df,Jul_2019_df,\n",
    "                       Aug_2019_df,Sep_2019_df,Oct_2019_df,Nov_2019_df,Dec_2019_df ]\n",
    "\n",
    "HayFev_df_2019 = pd.concat(HayFev_df_2019_list)\n",
    "\n",
    "HayFev_df_2019.to_csv('HayFev_df_2019.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HayFev_df_2019.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the cost of hardware, I am more inclined to look for workarounds such as these to facilitate and speed up personal projects. I hope this has been useful to others and I would love comments and feedback on my code. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
